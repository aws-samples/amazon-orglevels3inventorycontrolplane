AWSTemplateFormatVersion: '2010-09-09'
Description: Template automate creation of Athena table and populates inventory data

Parameters:
  InventoryBucketName:
    Type: String
    Description: Enter the name of S3 Bucket with centralized inventory (s3inventory-{region}-{accountId})
      
  WorkGroupName:
    Type: String
    Default: primary
    Description: The name of workgroup that will be used for Athena queries. Enter the unique name of your existing workgroup. Use 1 - 128 characters. (A-Z,a-z,0-9,_,-,.). 

Resources:
  LambdaInvokePermission:
    Type: 'AWS::Lambda::Permission'
    DependsOn:
      - LambdaFunction
    Properties:
      FunctionName: !GetAtt LambdaFunction.Arn
      Action: 'lambda:InvokeFunction'
      Principal: s3.amazonaws.com
      SourceAccount: !Ref 'AWS::AccountId'
      SourceArn: !Sub 'arn:aws:s3:::${InventoryBucketName}'
  
  LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: lambda-role
      AssumeRolePolicyDocument:
        Statement:
          - Action:
            - sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
              - lambda.amazonaws.com
        Version: 2012-10-17
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AWSLambdaExecute
        - arn:aws:iam::aws:policy/AmazonS3FullAccess
        - arn:aws:iam::aws:policy/AmazonAthenaFullAccess
        - arn:aws:iam::aws:policy/CloudWatchFullAccess
      Path: /

  Policy:
    Type: AWS::IAM::Policy
    DependsOn: LambdaRole
    Properties: 
      PolicyName: workgrouppolicy
      PolicyDocument:
        Version: "2012-10-17"
        Statement: 
          - 
            Sid: AllowWorkGroup
            Effect: "Allow"
            Action: 
              - '*'
            Resource: !Sub 'arn:aws:athena:${AWS::Region}:${AWS::AccountId}:workgroup/${WorkGroupName}'
      Roles:
        -
          !Ref LambdaRole
  
  LambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: lambda-function-inventory
      Description: LambdaFunction of python
      Runtime: python3.7
      Code:
        ZipFile: |
          import json
          import urllib.parse
          import boto3
          import time
          from botocore.exceptions import ClientError
          import sys
          import os

          print('Loading function')

          s3 = boto3.client('s3')
          client = boto3.client('athena')
          # Athena data base is by default set to default.
          DATABASE = 'default'

          S3_INPUT = os.environ['InventoryBucket']
          S3_INPUT = 's3://' + S3_INPUT +'/'
          print(S3_INPUT)

          # S3 source bucket to create a table from.
          # S3 bucket for storing Athena queries 
          # If you don't provide the WorkGroupName, the script will default to 'primary'. Please make sure primary workgroup is enabled.

          WorkGroupName = os.environ['LambdaWorkGroupName']

          RETRY_COUNT = 10

          #Running Athena queries and checking if provided workgroup is available in the region or not.
          def startQueryExecution(query, WorkGroupName):
              # if WorkGroupName is set to none or null, it will be set to 'primary'
              if WorkGroupName == '' or None:
                  WorkGroupName = 'primary'
              try: 
                  if WorkGroupName != 'primary':
                      WG = response = client.get_work_group(
                      WorkGroup=WorkGroupName
                      )
              except ClientError as e:
                  response = e.response
                  code = response['Error']['Code']
                  message = response['Error']['Message']
                  if code == 'InvalidRequestException':
                      print(f'Error in query, {code}:\n{message}')
                      sys.exit(1)
                  elif code == 'InternalServerException':
                      print(f'AWS {code}:\n{message}')
                      sys.exit(1)
                  else:
                      print (f"Please review review Athena Workgroup name and query. Workgroup must satisfy regular expression pattern: [a-zA-Z0-9._-]{1,128}")
                      raise e
              print (f'WorkGroupName is {WorkGroupName}')
              if WorkGroupName == 'primary':
                  response = client.start_query_execution(
                  QueryString=query,
                  QueryExecutionContext={ 'Database': DATABASE},
                  ResultConfiguration={'OutputLocation': f'{S3_INPUT}'},
                  )    
              else:
                  response = client.start_query_execution(
                  QueryString=query,
                  QueryExecutionContext={ 'Database': DATABASE},
                  #ResultConfiguration={'OutputLocation': f'{S3_OUTPUT}'},
                  WorkGroup=WorkGroupName
                  )
              #Wait for 5 sec
              time.sleep(5)
              query_execution_id = response['QueryExecutionId']
              #Get Execution Status
              for i in range(0, RETRY_COUNT):
                  #Get Query Execution
                  query_status = client.get_query_execution(
                      QueryExecutionId=query_execution_id
                  )
                  exec_status = query_status['QueryExecution']['Status']['State']
                  if exec_status == 'SUCCEEDED':
                      print(f'Status: {exec_status}')
                      break
                  elif exec_status == 'FAILED':
                      raise Exception(f'STATUS: {exec_status}')
                  else:
                      print(f'STATUS: {exec_status}')
                      time.sleep(i)

          def lambda_handler(event, context):
              #print("Received event: " + json.dumps(event, indent=2))

              # Get the object bucket and key from the event
              source_bucket = event['Records'][0]['s3']['bucket']['name']
              source_key = urllib.parse.unquote_plus(event['Records'][0]['s3']['object']['key'], encoding='utf-8')
              print (f'Source bucket is {source_bucket}')
              
              #source_bucket = 's3inventory-us-east-1-346203898508'
              #source_key = 'inventory/cloudtrail-awslogs-346203898508-crkmzfog-isengard-do-not-delete/cloudtrail-awslogs-346203898508-crkmzfog-isengard--Inventory/hive/dt=2022-10-18-00-00/symlink.txt'

              # Make sure the object path is a valid inventory symlink format.
              source_key_arr = source_key.split('/')
              if source_key.endswith('/symlink.txt') and len(source_key_arr) >= 5 and source_key_arr[-3] == 'hive':
                  # Extract inventory properties from path
                  inventory_bucket = source_key_arr[-5]
                  inventory_datetime = source_key_arr[-2][3:]
                  inventory_symlink = source_key_arr[-1]

                  # compute destination bucket and key using inventory properties
                  destination_bucket = source_bucket
                  # output format example: inventory/symlinks/dt=2021-08-18-00-00/bucket=my-bucket-name/symlink.txt
                  destination_key = 'centralizedinventory/symlinks/dt={}/bucketname={}/{}'.format(inventory_datetime, inventory_bucket, inventory_symlink)

                  # Copy symlink file to new location
                  copy_object={'Bucket':source_bucket,'Key':source_key}
                  try:
                      response = s3.copy_object(CopySource=copy_object, Bucket=destination_bucket, Key=destination_key)
                      time.sleep(5)
                      #client = boto3.client('athena')
                      query = f"""
                          CREATE EXTERNAL TABLE IF NOT EXISTS inventory(
                                  bucket string,
                                  key string,
                                  version_id string,
                                  is_latest boolean,
                                  is_delete_marker boolean,
                                  size bigint,
                                  last_modified_date timestamp,
                                  e_tag string,
                                  storage_class string,
                                  is_multipart_uploaded boolean,
                                  replication_status string,
                                  encryption_status string,
                                  object_lock_retain_until_date bigint,
                                  object_lock_mode string,
                                  object_lock_legal_hold_status string,
                                  intelligent_tiering_access_tier string,
                                  bucket_key_status string,
                                  checksum_algorithm string,
                                  ObjectAccessControlList string,
                                  ObjectOwner string
                          ) PARTITIONED BY (
                                  dt string,
                                  bucketname string
                          )
                          ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
                          STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat'
                          OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat'
                          LOCATION '{S3_INPUT}/centralizedinventory/symlinks/';
                      """
                      #Creating table
                      startQueryExecution(query, WorkGroupName)
                      #Adding new partitions
                      query=  "ALTER TABLE inventory ADD PARTITION (dt='{}', bucketname = '{}');".format(inventory_datetime, inventory_bucket)
                      print(f'Query is {query}')
                      startQueryExecution(query, WorkGroupName)
                      
                      return {
                          'statusCode': 200,
                          'body': json.dumps('File has been Successfully Copied from {}/{} to {}/{}'.format(source_bucket, source_key, destination_bucket, destination_key))
                      }
                  except Exception as e:
                      print(e)
                      print('Error getting object {} from bucket {}. Make sure they exist and your bucket is in the same region as this function.'.format(source_key, source_bucket))
                      raise e
                  

              return {
                  'statusCode': 200,
                  'body': json.dumps('Not a valid inventory symlink path: {}/{}'.format(source_bucket, source_key))
              }
      Handler: index.lambda_handler
      Environment:
        Variables:
          InventoryBucket: !Ref InventoryBucketName
          LambdaWorkGroupName: !Ref WorkGroupName
      MemorySize: 128
      Timeout: 600
      Role:
        Fn::GetAtt:
          - LambdaRole
          - Arn

  CustomResourceLambdaFunction:
    Type: 'AWS::Lambda::Function'
    Properties:
      Handler: index.lambda_handler
      Role: !GetAtt LambdaRole.Arn
      Code:
        ZipFile: |

            from __future__ import print_function
            import json
            import boto3
            import cfnresponse
            
            SUCCESS = "SUCCESS"
            FAILED = "FAILED"
            
            print('Loading function')
            s3 = boto3.resource('s3')
            
            def lambda_handler(event, context):
                print("Received event: " + json.dumps(event, indent=2))
                responseData={}
                try:
                    if event['RequestType'] == 'Delete':
                        print("Request Type:",event['RequestType'])
                        Bucket=event['ResourceProperties']['Bucket']
                        delete_notification(Bucket)
                        print("Sending response to custom resource after Delete")
                    elif event['RequestType'] == 'Create' or event['RequestType'] == 'Update':
                        print("Request Type:",event['RequestType'])
                        LambdaArn=event['ResourceProperties']['LambdaArn']
                        BucketLocation=event['ResourceProperties']['Bucket']
                        Bucket=BucketLocation[:]
                        add_notification(LambdaArn, Bucket)
                        responseData={'Bucket':Bucket}
                        print("Sending response to custom resource")
                    responseStatus = 'SUCCESS'
                except Exception as e:
                    print('Failed to process:', e)
                    responseStatus = 'FAILED'
                    responseData = {'Failure': 'Something bad happened.'}
                cfnresponse.send(event, context, responseStatus, responseData, "CustomResourcePhysicalID")

            def add_notification(LambdaArn, Bucket):
                bucket_notification = s3.BucketNotification(Bucket)
                response = bucket_notification.put(
                  NotificationConfiguration={
                    'LambdaFunctionConfigurations': [
                      {
                          'LambdaFunctionArn': LambdaArn,
                          'Events': [
                              's3:ObjectCreated:*'
                          ],                
                          'Filter': {
                            'Key': {
                              'FilterRules': [
                                 {
                                   'Name': 'prefix',
                                   'Value': 'inventory/'
                                 },
                                 {
                                   'Name': 'suffix',
                                   'Value': '/symlink.txt'
                                 }
                               ]
                             }
                           }
                      }
                    ]
                  }
                )
                print("Put request completed....")
              
            def delete_notification(Bucket):
                bucket_notification = s3.BucketNotification(Bucket)
                response = bucket_notification.put(
                    NotificationConfiguration={}
                )
                print("Delete request completed....")
      Runtime: python3.9
      Timeout: 50

  LambdaTrigger:
    Type: 'Custom::LambdaTrigger'
    DependsOn: LambdaInvokePermission
    Properties:
      ServiceToken: !GetAtt CustomResourceLambdaFunction.Arn
      LambdaArn: !GetAtt LambdaFunction.Arn
      Bucket: !Ref InventoryBucketName
